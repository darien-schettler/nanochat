{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Level 3: Architecture & Performance\n\n",
        "**Objective:** Fix bugs in rotary embeddings, KV cache, MQA, memory management, and tool use.\n\n",
        "**Acceptance Criteria:**\n",
        "- All tests in `tests/test_level3.py` pass\n",
        "- Model handles long sequences correctly\n",
        "- KV cache produces consistent outputs\n",
        "- MQA works with n_head != n_kv_head\n",
        "- Memory usage stays bounded\n",
        "- Tool outputs appear in correct order\n\n",
        "**Time estimate:** 2-3 hours"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import gc\n\n",
        "repo_root = Path.cwd()\n",
        "if str(repo_root) not in sys.path:\n",
        "    sys.path.append(str(repo_root))\n\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Device: {device}\")\n\n",
        "os.environ[\"NANOCHAT_BASE_DIR\"] = os.path.join(repo_root, \".cache_level3\")\n",
        "os.makedirs(os.environ[\"NANOCHAT_BASE_DIR\"], exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nanochat.tokenizer import RustBPETokenizer\n",
        "from nanochat.gpt import GPT, GPTConfig\n",
        "from nanochat.engine import Engine, KVCache\n\n",
        "# Create tokenizer\n",
        "tokenizer_dir = Path(os.environ[\"NANOCHAT_BASE_DIR\"]) / \"tokenizer\"\n",
        "if not tokenizer_dir.exists():\n",
        "    texts = [\"test\"] * 1000\n",
        "    tokenizer = RustBPETokenizer.train_from_iterator(iter(texts), vocab_size=512)\n",
        "    tokenizer.save(str(tokenizer_dir))\n",
        "else:\n",
        "    tokenizer = RustBPETokenizer.from_directory(str(tokenizer_dir))\n\n",
        "# Create model with MQA\n",
        "config = GPTConfig(\n",
        "    sequence_len=256,\n",
        "    vocab_size=512,\n",
        "    n_layer=4,\n",
        "    n_head=8,\n",
        "    n_kv_head=4,  # MQA: fewer KV heads than query heads\n",
        "    n_embd=256,\n",
        ")\n\n",
        "model = GPT(config)\n",
        "model.init_weights()\n",
        "model = model.to(device)\n",
        "print(f\"Model: {sum(p.numel() for p in model.parameters()):,} params\")\n",
        "print(f\"MQA config: {config.n_head} query heads, {config.n_kv_head} KV heads\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 1: Rotary Embeddings at Different Positions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test model at different sequence positions\n",
        "test_seq = torch.randint(0, 100, (1, 20), dtype=torch.int32).to(device)\n\n",
        "outputs_at_positions = []\n",
        "for pos in [0, 50, 100, 150, 200]:\n",
        "    with torch.no_grad():\n",
        "        T = test_seq.size(1)\n",
        "        cos_sin = model.cos[:, pos:pos+T], model.sin[:, pos:pos+T]\n",
        "        \n",
        "        x = model.transformer.wte(test_seq)\n",
        "        x = torch.nn.functional.rms_norm(x, (x.size(-1),))\n",
        "        attn_out = model.transformer.h[0].attn(x, cos_sin, kv_cache=None)\n",
        "        \n",
        "        outputs_at_positions.append(attn_out[0, 0, :5].cpu().numpy())\n\n",
        "# Plot outputs\n",
        "plt.figure(figsize=(10, 5))\n",
        "for i, out in enumerate(outputs_at_positions):\n",
        "    plt.plot(out, label=f'Pos {[0,50,100,150,200][i]}', marker='o')\n",
        "plt.xlabel('Dimension')\n",
        "plt.ylabel('Output')\n",
        "plt.title('Attention Output at Different Positions')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n\n",
        "# Acceptance: outputs should have similar variance across positions\n",
        "variances = [np.var(out) for out in outputs_at_positions]\n",
        "print(f\"Variances: {variances}\")\n",
        "assert variances[-1] > variances[0] * 0.1, f\"FAIL: Output degrades at high positions\"\n",
        "print(\"\u2713 Test 1 passed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 2: KV Cache Consistency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test KV cache produces consistent outputs\n",
        "engine = Engine(model, tokenizer)\n",
        "prompt = \"test prompt\"\n",
        "prompt_tokens = tokenizer.encode(prompt, prepend=\"<|bos|>\")\n\n",
        "# Generate with cache\n",
        "generated_tokens = []\n",
        "for token_col, _ in engine.generate(prompt_tokens, max_tokens=30):\n",
        "    generated_tokens.append(token_col[0])\n\n",
        "generated_text = tokenizer.decode(prompt_tokens + generated_tokens)\n",
        "print(f\"Generated: '{generated_text}'\")\n\n",
        "# Acceptance: should generate without errors\n",
        "assert len(generated_tokens) == 30, f\"FAIL: Generated {len(generated_tokens)} tokens, expected 30\"\n",
        "# Check for excessive repetition (sign of cache corruption)\n",
        "unique_tokens = len(set(generated_tokens[-10:]))\n",
        "assert unique_tokens >= 3, f\"FAIL: Too much repetition in last 10 tokens ({unique_tokens} unique)\"\n",
        "print(\"\u2713 Test 2 passed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 3: MQA Shape Compatibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test MQA with mismatched head counts\n",
        "attn = model.transformer.h[0].attn\n",
        "B, T = 2, 10\n",
        "x = torch.randn(B, T, config.n_embd).to(device)\n\n",
        "# Get Q, K, V\n",
        "q = attn.c_q(x).view(B, T, config.n_head, config.n_embd // config.n_head)\n",
        "k = attn.c_k(x).view(B, T, config.n_kv_head, config.n_embd // config.n_head)\n",
        "v = attn.c_v(x).view(B, T, config.n_kv_head, config.n_embd // config.n_head)\n\n",
        "print(f\"Q shape: {q.shape} ({config.n_head} heads)\")\n",
        "print(f\"K shape: {k.shape} ({config.n_kv_head} heads)\")\n",
        "print(f\"V shape: {v.shape} ({config.n_kv_head} heads)\")\n\n",
        "# Test forward pass with MQA\n",
        "try:\n",
        "    cos_sin = model.cos[:, :T], model.sin[:, :T]\n",
        "    output = attn(x, cos_sin, kv_cache=None)\n",
        "    assert output.shape == (B, T, config.n_embd), f\"FAIL: Wrong output shape {output.shape}\"\n",
        "    print(\"\u2713 Test 3 passed\")\n",
        "except Exception as e:\n",
        "    print(f\"\u2717 Test 3 failed: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 4: Memory Management"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test memory usage across multiple generations\n",
        "if device == 'cuda':\n",
        "    mem_usage = []\n",
        "    \n",
        "    for i in range(5):\n",
        "        engine = Engine(model, tokenizer)\n",
        "        tokens = tokenizer.encode(f\"test {i}\", prepend=\"<|bos|>\")\n",
        "        result, _ = engine.generate_batch(tokens, max_tokens=50)\n",
        "        \n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        mem = torch.cuda.memory_allocated() / 1024**2\n",
        "        mem_usage.append(mem)\n",
        "        print(f\"Gen {i+1}: {mem:.2f} MB\")\n",
        "    \n",
        "    mem_increase = mem_usage[-1] - mem_usage[0]\n",
        "    print(f\"\\nMemory increase: {mem_increase:.2f} MB\")\n",
        "    \n",
        "    assert mem_increase < 50, f\"FAIL: Memory leak detected ({mem_increase:.2f} MB increase)\"\n",
        "    print(\"\u2713 Test 4 passed\")\n",
        "else:\n",
        "    print(\"\u2298 Test 4 skipped (requires CUDA)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 5: Tool Use Token Order"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test calculator tool output ordering\n",
        "conversation = {\n",
        "    \"messages\": [\n",
        "        {\"role\": \"user\", \"content\": \"What is 5+3?\"},\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"text\", \"text\": \"Let me calculate: \"},\n",
        "                {\"type\": \"python\", \"text\": \"5+3\"},\n",
        "                {\"type\": \"python_output\", \"text\": \"8\"},\n",
        "                {\"type\": \"text\", \"text\": \" The answer is 8.\"}\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "}\n\n",
        "ids, mask = tokenizer.render_conversation(conversation)\n",
        "print(f\"Conversation: {len(ids)} tokens\")\n\n",
        "# Find special tokens\n",
        "special_tokens = []\n",
        "for i, token_id in enumerate(ids):\n",
        "    decoded = tokenizer.decode([token_id])\n",
        "    if decoded.startswith('<|') and decoded.endswith('|>'):\n",
        "        special_tokens.append((i, decoded))\n\n",
        "print(\"\\nSpecial tokens:\")\n",
        "for pos, tok in special_tokens:\n",
        "    print(f\"  {pos}: {tok}\")\n\n",
        "# Acceptance: output_start should come before output_end\n",
        "output_tokens = [(pos, tok) for pos, tok in special_tokens if 'output' in tok]\n",
        "if len(output_tokens) >= 2:\n",
        "    assert '<|output_start|>' in output_tokens[0][1], \"FAIL: output_start should come first\"\n",
        "    assert '<|output_end|>' in output_tokens[1][1], \"FAIL: output_end should come after output_start\"\n",
        "    print(\"\u2713 Test 5 passed\")\n",
        "else:\n",
        "    print(\"\u2298 Test 5: Insufficient output tokens to verify order\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n\n",
        "All tests passed! Architecture and performance issues resolved."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}