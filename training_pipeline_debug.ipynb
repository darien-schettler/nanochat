{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83d\udc1b Nanochat Bug Hunt: Training Pipeline\n\nWelcome to the intermediate debugging challenge! In this notebook, you'll fix 4 training pipeline bugs:\n\n1. **Gradient Accumulation Bug**: Loss explodes with large batch sizes\n2. **Learning Rate Bug**: Model doesn't learn due to broken warmup\n3. **Optimizer Misconfig**: Wrong parameters assigned to optimizers\n4. **SFT Masking Bug**: Model learns to repeat special tokens\n\nThis notebook assumes you have a small pretrained base model to work with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup\nimport os\nimport sys\nimport torch\nimport numpy as np\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport importlib\n\n# Add nanochat to path\nrepo_root = Path.cwd()\nif str(repo_root) not in sys.path:\n    sys.path.append(str(repo_root))\n\n# Device selection\ndevice = 'cuda' if torch.cuda.is_available() else 'mps' if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available() else 'cpu'\nprint(f\"Using device: {device}\")\n\n# Set up cache directory\nos.environ[\"NANOCHAT_BASE_DIR\"] = os.path.join(repo_root, \".cache_medium\")\nos.makedirs(os.environ[\"NANOCHAT_BASE_DIR\"], exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Create a Small Model and Dataset\n\nWe'll create a minimal setup to test training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# First, create a tokenizer (or load if exists)\nfrom nanochat.tokenizer import RustBPETokenizer\n\ntokenizer_dir = Path(os.environ[\"NANOCHAT_BASE_DIR\"]) / \"tokenizer\"\n\nif tokenizer_dir.exists():\n    print(\"Loading existing tokenizer...\")\n    tokenizer = RustBPETokenizer.from_directory(str(tokenizer_dir))\nelse:\n    print(\"Creating new tokenizer...\")\n    # Create training data\n    texts = [\n        \"The quick brown fox jumps over the lazy dog.\",\n        \"Machine learning is transforming the world.\",\n        \"Python is a versatile programming language.\",\n        \"Artificial intelligence powers modern technology.\",\n    ] * 200\n    \n    def text_iterator():\n        for text in texts:\n            yield text\n    \n    tokenizer = RustBPETokenizer.train_from_iterator(text_iterator(), vocab_size=1024)\n    tokenizer.save(str(tokenizer_dir))\n\nprint(f\"Tokenizer vocab size: {tokenizer.get_vocab_size()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a tiny model\nfrom nanochat.gpt import GPT, GPTConfig\n\nconfig = GPTConfig(\n    sequence_len=128,\n    vocab_size=tokenizer.get_vocab_size(),\n    n_layer=4,      # Small model\n    n_head=4,\n    n_kv_head=4,\n    n_embd=128,\n)\n\nmodel = GPT(config)\nmodel.init_weights()\nmodel = model.to(device)\nprint(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Test Gradient Accumulation - Find Bug #1\n\nLet's simulate training with gradient accumulation and see what happens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create simple training data\ndef get_batch(batch_size=2, seq_len=64):\n    \"\"\"Generate random batch for testing\"\"\"\n    # Random tokens\n    inputs = torch.randint(0, tokenizer.get_vocab_size(), (batch_size, seq_len), dtype=torch.int32).to(device)\n    # Targets are shifted inputs (simplified)\n    targets = torch.randint(0, tokenizer.get_vocab_size(), (batch_size, seq_len), dtype=torch.int64).to(device)\n    return inputs, targets\n\n# Test training with gradient accumulation\n# Simulating the bug from base_train.py\ndef train_with_grad_accum(model, steps=20, grad_accum_steps=4):\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n    losses = []\n    \n    for step in range(steps):\n        total_loss = 0\n        \n        # Gradient accumulation loop (mimicking base_train.py)\n        for micro_step in range(grad_accum_steps):\n            x, y = get_batch()\n            loss = model(x, y)\n            \n            # BUG 1: Not normalizing loss!\n            # loss = loss / grad_accum_steps  # This line is missing!\n            loss.backward()  # Accumulating unnormalized losses\n            \n            total_loss += loss.item()\n        \n        # Step optimizer\n        optimizer.step()\n        optimizer.zero_grad()\n        \n        avg_loss = total_loss / grad_accum_steps\n        losses.append(avg_loss)\n        \n        if step % 5 == 0:\n            print(f\"Step {step}: avg loss = {avg_loss:.4f}\")\n    \n    return losses\n\nprint(\"Training with gradient accumulation (buggy)...\")\nlosses_buggy = train_with_grad_accum(model.clone(), grad_accum_steps=8)\n\nplt.figure(figsize=(8, 4))\nplt.plot(losses_buggy)\nplt.xlabel('Step')\nplt.ylabel('Loss')\nplt.title('Training Loss with Bug #1 (Unnormalized Gradient Accumulation)')\nplt.show()\n\nif losses_buggy[-1] > losses_buggy[0] * 2:\n    print(\"\\\\n\u274c BUG #1 DETECTED! Loss is exploding!\")\n    print(\"\ud83d\udca1 The gradients are being accumulated without normalization.\")\n    print(\"\ud83d\udca1 Check base_train.py around line 271 where loss.backward() is called.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fix Bug #1: Gradient Accumulation\n\nGo to `scripts/base_train.py` and uncomment the line that normalizes loss:\n```python\nloss = loss / grad_accum_steps  # each .backward() is a grad sum => normalize loss here\n```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test with fix\ndef train_with_grad_accum_fixed(model, steps=20, grad_accum_steps=4):\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n    losses = []\n    \n    for step in range(steps):\n        total_loss = 0\n        \n        for micro_step in range(grad_accum_steps):\n            x, y = get_batch()\n            loss = model(x, y)\n            \n            # FIXED: Normalize loss\n            loss = loss / grad_accum_steps\n            loss.backward()\n            \n            total_loss += loss.item() * grad_accum_steps  # Scale back for logging\n        \n        optimizer.step()\n        optimizer.zero_grad()\n        \n        avg_loss = total_loss / grad_accum_steps\n        losses.append(avg_loss)\n    \n    return losses\n\n# Reinitialize model\nmodel_fixed = GPT(config)\nmodel_fixed.init_weights()\nmodel_fixed = model_fixed.to(device)\n\nprint(\"Training with gradient accumulation (fixed)...\")\nlosses_fixed = train_with_grad_accum_fixed(model_fixed, grad_accum_steps=8)\n\nplt.figure(figsize=(8, 4))\nplt.plot(losses_fixed)\nplt.xlabel('Step')\nplt.ylabel('Loss')\nplt.title('Training Loss with Bug #1 Fixed')\nplt.show()\n\nprint(\"\u2705 Bug #1 FIXED! Loss is now stable with gradient accumulation!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Test Learning Rate Schedule - Find Bug #2\n\nNow let's test the learning rate warmup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the learning rate schedule from base_train.py\ndef get_lr_multiplier_buggy(it, num_iterations=1000, warmup_ratio=0.1):\n    \"\"\"Buggy LR scheduler from base_train.py\"\"\"\n    warmup_iters = round(warmup_ratio * num_iterations)\n    if it < warmup_iters:\n        # BUG: Warmup is divided by 100x too much!\n        return (it + 1) / (warmup_iters * 100)\n    else:\n        return 1.0\n\n# Plot the learning rate schedule\nnum_iters = 200\nwarmup_ratio = 0.1\nsteps = range(num_iters)\nlr_multipliers = [get_lr_multiplier_buggy(step, num_iters, warmup_ratio) for step in steps]\n\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nplt.plot(steps, lr_multipliers)\nplt.axhline(y=1.0, color='r', linestyle='--', label='Target LR')\nplt.xlabel('Step')\nplt.ylabel('LR Multiplier')\nplt.title('Buggy Learning Rate Schedule')\nplt.legend()\nplt.ylim(0, 1.2)\n\n# Zoom in on warmup\nplt.subplot(1, 2, 2)\nwarmup_steps = int(warmup_ratio * num_iters)\nplt.plot(steps[:warmup_steps*2], lr_multipliers[:warmup_steps*2])\nplt.axvline(x=warmup_steps, color='g', linestyle='--', label='End of warmup')\nplt.xlabel('Step')\nplt.ylabel('LR Multiplier')\nplt.title('Warmup Phase (Zoomed)')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\\\n\u274c BUG #2 DETECTED! Learning rate at step 10: {lr_multipliers[10]:.6f}\")\nprint(f\"Expected ~0.5, but got {lr_multipliers[10]:.6f} (100x too small!)\")\nprint(\"\ud83d\udca1 Check get_lr_multiplier() in base_train.py around line 160\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fix Bug #2: Learning Rate Schedule\n\nFix the warmup calculation in `scripts/base_train.py`:\n```python\nreturn (it + 1) / warmup_iters  # Remove the * 100\n```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fixed LR schedule\ndef get_lr_multiplier_fixed(it, num_iterations=1000, warmup_ratio=0.1):\n    \"\"\"Fixed LR scheduler\"\"\"\n    warmup_iters = round(warmup_ratio * num_iterations)\n    if it < warmup_iters:\n        return (it + 1) / warmup_iters  # Fixed!\n    else:\n        return 1.0\n\n# Plot fixed schedule\nlr_multipliers_fixed = [get_lr_multiplier_fixed(step, num_iters, warmup_ratio) for step in steps]\n\nplt.figure(figsize=(8, 4))\nplt.plot(steps, lr_multipliers_fixed, label='Fixed')\nplt.plot(steps, lr_multipliers, label='Buggy', alpha=0.5)\nplt.axhline(y=1.0, color='r', linestyle='--', label='Target LR')\nplt.xlabel('Step')\nplt.ylabel('LR Multiplier')\nplt.title('Learning Rate Schedule Comparison')\nplt.legend()\nplt.show()\n\nprint(\"\u2705 Bug #2 FIXED! Learning rate warmup now works correctly!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Test Optimizer Assignment - Find Bug #3\n\nLet's check if the optimizers are assigned to the correct parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the optimizer setup\nprint(\"Testing optimizer setup...\\\\n\")\n\n# Get the optimizers from the model\noptimizers = model.setup_optimizers()\nadamw_opt, muon_opt = optimizers\n\n# Check what parameters are in each optimizer\nprint(\"AdamW optimizer parameter groups:\")\nfor i, group in enumerate(adamw_opt.param_groups):\n    params = group['params']\n    print(f\"  Group {i}: {len(params)} parameters\")\n    # Check what these parameters are\n    for p in params[:2]:  # Just check first 2\n        for name, param in model.named_parameters():\n            if param is p:\n                print(f\"    - {name}\")\n                break\n\nprint(\"\\\\nMuon optimizer parameters:\")\nmuon_params = muon_opt.param_groups[0]['params']\nprint(f\"  {len(muon_params)} parameters\")\nfor p in muon_params[:2]:  # Just check first 2\n    for name, param in model.named_parameters():\n        if param is p:\n            print(f\"    - {name}\")\n            break\n\n# Check if bug exists\nprint(\"\\\\n\u274c BUG #3 DETECTED! Optimizer assignment is wrong!\")\nprint(\"\ud83d\udca1 Matrix parameters (transformer.h.*) should use Muon optimizer\")\nprint(\"\ud83d\udca1 But they're assigned to AdamW instead!\")\nprint(\"\ud83d\udca1 Check setup_optimizers() in nanochat/gpt.py around line 227\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fix Bug #3: Optimizer Assignment\n\nIn `nanochat/gpt.py`, fix the parameter assignments:\n- AdamW should get: `lm_head_params` and `embedding_params`\n- Muon should get: `matrix_params`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# After fixing, reload the module\nimport nanochat.gpt\nimportlib.reload(nanochat.gpt)\nfrom nanochat.gpt import GPT, GPTConfig\n\n# Create new model and check\nmodel_fixed = GPT(config)\nmodel_fixed.init_weights()\nmodel_fixed = model_fixed.to(device)\n\n# This would work after the fix\nprint(\"After fixing gpt.py, the optimizers should be correctly assigned.\")\nprint(\"\u2705 Bug #3 will be FIXED when you update the code!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Test SFT Masking - Find Bug #4\n\nFinally, let's test the SFT masking logic for conversation training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a sample conversation\nconversation = {\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n        {\"role\": \"assistant\", \"content\": \"I'm doing well, thank you!\"}\n    ]\n}\n\n# Tokenize the conversation\nids, mask = tokenizer.render_conversation(conversation)\n\nprint(\"Tokenized conversation:\")\nprint(f\"IDs: {ids[:20]}... (showing first 20)\")\nprint(f\"Mask: {mask[:20]}... (showing first 20)\")\nprint(f\"\\\\nMask values: 0 = don't train, 1 = train on this token\")\n\n# Visualize what tokens we're training on\nprint(\"\\\\nVisualization (GREEN = train, RED = don't train):\")\nprint(tokenizer.visualize_tokenization(ids[:50], mask[:50]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulate the buggy SFT data processing\ndef process_sft_batch_buggy(ids, mask):\n    \"\"\"Simulate the buggy masking from chat_sft.py\"\"\"\n    ids_tensor = torch.tensor(ids, dtype=torch.long)\n    mask_tensor = torch.tensor(mask[1:], dtype=torch.long)  # Skip BOS mask\n    \n    # Create targets\n    targets = ids_tensor[1:].clone()\n    \n    # BUG: Inverted mask logic!\n    targets[mask_tensor == 1] = -1  # Bug: masking where we SHOULD train!\n    \n    return targets\n\n# Process with bug\ntargets_buggy = process_sft_batch_buggy(ids, mask)\n\n# Count how many tokens we're training on\nnum_train_tokens = (targets_buggy != -1).sum().item()\ntotal_tokens = len(targets_buggy)\n\nprint(f\"\\\\nWith buggy masking:\")\nprint(f\"Training on {num_train_tokens}/{total_tokens} tokens\")\nprint(f\"That's only {100*num_train_tokens/total_tokens:.1f}% of tokens!\")\n\n# Check what we're training on\ntrain_indices = (targets_buggy != -1).nonzero().squeeze().tolist()\nif isinstance(train_indices, int):\n    train_indices = [train_indices]\nprint(f\"\\\\nTraining on tokens at positions: {train_indices[:10]}...\")\n\nprint(\"\\\\n\u274c BUG #4 DETECTED! Mask logic is inverted!\")\nprint(\"\ud83d\udca1 We're training on user messages instead of assistant messages!\")\nprint(\"\ud83d\udca1 Check chat_sft.py around line 114 where mask is applied\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fix Bug #4: SFT Masking\n\nIn `scripts/chat_sft.py`, fix the mask logic:\n```python\nrow_targets[mask_tensor == 0] = -1  # Mask where mask is 0, not 1!\n```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fixed masking\ndef process_sft_batch_fixed(ids, mask):\n    \"\"\"Fixed SFT masking\"\"\"\n    ids_tensor = torch.tensor(ids, dtype=torch.long)\n    mask_tensor = torch.tensor(mask[1:], dtype=torch.long)\n    \n    targets = ids_tensor[1:].clone()\n    \n    # FIXED: Correct mask logic\n    targets[mask_tensor == 0] = -1  # Mask where we should NOT train\n    \n    return targets\n\n# Process with fix\ntargets_fixed = process_sft_batch_fixed(ids, mask)\nnum_train_tokens_fixed = (targets_fixed != -1).sum().item()\n\nprint(f\"With fixed masking:\")\nprint(f\"Training on {num_train_tokens_fixed}/{total_tokens} tokens\")\nprint(f\"That's {100*num_train_tokens_fixed/total_tokens:.1f}% of tokens!\")\n\n# Visualize what we're training on now\nprint(\"\\\\n\u2705 Bug #4 FIXED! Now training on assistant responses only!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary\nprint(\"\ud83c\udf89 Congratulations! You've debugged the training pipeline!\\\\n\")\nprint(\"Summary of fixes:\")\nprint(\"1. \u2705 Gradient accumulation: Added loss normalization\")\nprint(\"2. \u2705 Learning rate warmup: Removed factor of 100\")\nprint(\"3. \u2705 Optimizer assignment: Matrix params \u2192 Muon, embeddings \u2192 AdamW\")\nprint(\"4. \u2705 SFT masking: Fixed inverted mask logic\")\nprint(\"\\\\nThese bugs would have caused:\")\nprint(\"- Unstable training with large batch sizes\")\nprint(\"- Extremely slow initial learning\")\nprint(\"- Poor optimization of different parameter types\")\nprint(\"- Model learning wrong conversation patterns\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}