{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83d\udc1b Nanochat Bug Hunt: Tokenizer & Data Loading\n",
        "\n",
        "Welcome to the first debugging challenge! In this notebook, you'll find and fix three bugs:\n",
        "\n",
        "1. **Tokenizer Bug**: BOS tokens aren't being prepended\n",
        "2. **Data Type Bug**: Wrong tensor dtype causing CUDA errors\n",
        "3. **Off-by-One Bug**: Model learns to predict current token instead of next\n",
        "\n",
        "Let's start by setting up a tiny training run and see what goes wrong!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup: imports and environment\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Add nanochat to path\n",
        "repo_root = Path.cwd()\n",
        "if str(repo_root) not in sys.path:\n",
        "    sys.path.append(str(repo_root))\n",
        "\n",
        "# Device selection\n",
        "device = 'cuda' if torch.cuda.is_available() else 'mps' if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set up minimal cache directory\n",
        "os.environ[\"NANOCHAT_BASE_DIR\"] = os.path.join(repo_root, \".cache_debug\")\n",
        "os.makedirs(os.environ[\"NANOCHAT_BASE_DIR\"], exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Create a Tiny Dataset and Train a Small Tokenizer\n",
        "\n",
        "First, let's create a minimal dataset and train a tokenizer to work with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a tiny text dataset\n",
        "tiny_texts = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"To be or not to be, that is the question.\",\n",
        "    \"Hello world! This is a test of the tokenizer.\",\n",
        "    \"Machine learning is fascinating and powerful.\",\n",
        "    \"Python is a great programming language for AI.\",\n",
        "] * 100  # Repeat to get enough data for BPE\n",
        "\n",
        "# Save as a simple text file\n",
        "data_path = Path(os.environ[\"NANOCHAT_BASE_DIR\"]) / \"tiny_data.txt\"\n",
        "with open(data_path, 'w') as f:\n",
        "    f.write('\\\\n'.join(tiny_texts))\n",
        "\n",
        "print(f\"Created dataset with {len(tiny_texts)} lines\")\n",
        "print(f\"First line: {tiny_texts[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train a tiny tokenizer\n",
        "from nanochat.tokenizer import RustBPETokenizer\n",
        "\n",
        "# Create an iterator from our text\n",
        "def text_iterator():\n",
        "    with open(data_path, 'r') as f:\n",
        "        for line in f:\n",
        "            yield line.strip()\n",
        "\n",
        "# Train tokenizer with small vocab\n",
        "print(\"Training tokenizer...\")\n",
        "tokenizer = RustBPETokenizer.train_from_iterator(text_iterator(), vocab_size=512)\n",
        "print(f\"Tokenizer vocab size: {tokenizer.get_vocab_size()}\")\n",
        "\n",
        "# Save it\n",
        "tokenizer_dir = Path(os.environ[\"NANOCHAT_BASE_DIR\"]) / \"tokenizer\"\n",
        "tokenizer.save(str(tokenizer_dir))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Test the Tokenizer - Find Bug #1\n",
        "\n",
        "Let's test if the tokenizer correctly prepends BOS tokens when asked."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test tokenizer encoding with BOS token\n",
        "test_text = \"Hello world!\"\n",
        "bos_token_id = tokenizer.get_bos_token_id()\n",
        "print(f\"BOS token ID: {bos_token_id}\")\n",
        "\n",
        "# Encode without prepending\n",
        "tokens_no_bos = tokenizer.encode(test_text)\n",
        "print(f\"\\\\nWithout BOS: {tokens_no_bos}\")\n",
        "print(f\"Decoded: {tokenizer.decode(tokens_no_bos)}\")\n",
        "\n",
        "# Encode WITH prepending BOS\n",
        "tokens_with_bos = tokenizer.encode(test_text, prepend=\"<|bos|>\")\n",
        "print(f\"\\\\nWith BOS (should start with {bos_token_id}): {tokens_with_bos}\")\n",
        "print(f\"Decoded: {tokenizer.decode(tokens_with_bos)}\")\n",
        "\n",
        "# \ud83d\udc1b BUG CHECK: Does the token list start with BOS?\n",
        "if tokens_with_bos[0] != bos_token_id:\n",
        "    print(\"\\\\n\u274c BUG FOUND! BOS token is not being prepended!\")\n",
        "    print(\"\ud83d\udca1 Hint: Check the encode() method in tokenizer.py\")\n",
        "else:\n",
        "    print(\"\\\\n\u2705 BOS token correctly prepended!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fix Bug #1: BOS Token Prepending\n",
        "\n",
        "If you found the bug above, go fix it in `nanochat/tokenizer.py`!\n",
        "\n",
        "Look for the `encode()` method in the `RustBPETokenizer` class. The prepending logic has been commented out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# After fixing, reload the tokenizer module and test again\n",
        "import importlib\n",
        "import nanochat.tokenizer\n",
        "importlib.reload(nanochat.tokenizer)\n",
        "from nanochat.tokenizer import RustBPETokenizer\n",
        "\n",
        "# Reload tokenizer\n",
        "tokenizer = RustBPETokenizer.from_directory(str(tokenizer_dir))\n",
        "\n",
        "# Test again\n",
        "tokens_with_bos = tokenizer.encode(test_text, prepend=\"<|bos|>\")\n",
        "if tokens_with_bos[0] == bos_token_id:\n",
        "    print(\"\u2705 Bug #1 FIXED! BOS token now prepended correctly!\")\n",
        "else:\n",
        "    print(\"\u274c Bug #1 still present. Check your fix!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Create a Tiny Model and Try Training - Find Bug #2\n",
        "\n",
        "Now let's create a small model and attempt to train it. This will reveal the data type bug."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a tiny GPT model\n",
        "from nanochat.gpt import GPT, GPTConfig\n",
        "\n",
        "# Very small config for quick testing\n",
        "config = GPTConfig(\n",
        "    sequence_len=64,\n",
        "    vocab_size=tokenizer.get_vocab_size(),\n",
        "    n_layer=2,      # Just 2 layers\n",
        "    n_head=2,       # 2 heads\n",
        "    n_kv_head=2,    # Same as n_head (no MQA)\n",
        "    n_embd=64,      # Small embedding dim\n",
        ")\n",
        "\n",
        "model = GPT(config)\n",
        "model.init_weights()\n",
        "model = model.to(device)\n",
        "print(f\"Model created with {sum(p.numel() for p in model.parameters()):,} parameters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a simple data loader\n",
        "# First, let's tokenize our data and save it\n",
        "import pickle\n",
        "\n",
        "# Tokenize all our texts\n",
        "all_tokens = []\n",
        "for text in tiny_texts:\n",
        "    tokens = tokenizer.encode(text, prepend=\"<|bos|>\")\n",
        "    all_tokens.extend(tokens)\n",
        "\n",
        "# Save as a simple binary file\n",
        "tokens_path = Path(os.environ[\"NANOCHAT_BASE_DIR\"]) / \"tokens.bin\"\n",
        "tokens_array = np.array(all_tokens, dtype=np.uint16)\n",
        "tokens_array.tofile(tokens_path)\n",
        "\n",
        "print(f\"Total tokens: {len(all_tokens)}\")\n",
        "print(f\"Saved to: {tokens_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a simple batch getter\n",
        "def get_batch(batch_size=4, seq_len=64):\n",
        "    \"\"\"Get a batch of data for training\"\"\"\n",
        "    # Load tokens\n",
        "    data = np.fromfile(tokens_path, dtype=np.uint16)\n",
        "    \n",
        "    # Random starting positions\n",
        "    ix = torch.randint(len(data) - seq_len - 1, (batch_size,))\n",
        "    \n",
        "    # Create batch\n",
        "    batch_tokens = []\n",
        "    for i in ix:\n",
        "        batch_tokens.append(data[i:i+seq_len+1].tolist())\n",
        "    \n",
        "    # Now use our dataloader logic\n",
        "    from nanochat.dataloader import tokenizing_distributed_data_loader\n",
        "    \n",
        "    # We'll directly use the dataloader's batch creation logic\n",
        "    from collections import deque\n",
        "    token_buffer = deque()\n",
        "    for tokens in batch_tokens:\n",
        "        token_buffer.extend(tokens)\n",
        "    \n",
        "    B, T = batch_size, seq_len\n",
        "    needed_tokens = B * T + 1\n",
        "    \n",
        "    # This mimics the dataloader logic where the bug is\n",
        "    tokens = [token_buffer.popleft() for _ in range(needed_tokens)]\n",
        "    scratch = torch.tensor(tokens, dtype=torch.int64, pin_memory=(device == \"cuda\"))\n",
        "    \n",
        "    # BUG 2: Wrong dtype - using int64 for inputs instead of int32\n",
        "    inputs_cpu = scratch[:-1].to(dtype=torch.int64)  # Should be int32!\n",
        "    targets_cpu = scratch[1:]\n",
        "    \n",
        "    # Reshape to 2D and move to device\n",
        "    try:\n",
        "        inputs = inputs_cpu.view(B, T).to(device=device, dtype=torch.int32, non_blocking=True)\n",
        "        targets = targets_cpu.view(B, T).to(device=device, dtype=torch.int64, non_blocking=True)\n",
        "        return inputs, targets\n",
        "    except Exception as e:\n",
        "        print(f\"\u274c Error creating batch: {e}\")\n",
        "        print(f\"\ud83d\udca1 Hint: Check the dtype conversions in the dataloader!\")\n",
        "        print(f\"Input CPU dtype: {inputs_cpu.dtype}\")\n",
        "        print(f\"Target CPU dtype: {targets_cpu.dtype}\")\n",
        "        raise e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Try to get a batch - this should reveal Bug #2\n",
        "try:\n",
        "    inputs, targets = get_batch()\n",
        "    print(f\"\u2705 Batch created successfully!\")\n",
        "    print(f\"Inputs shape: {inputs.shape}, dtype: {inputs.dtype}\")\n",
        "    print(f\"Targets shape: {targets.shape}, dtype: {targets.dtype}\")\n",
        "except RuntimeError as e:\n",
        "    print(f\"\\\\n\ud83d\udc1b BUG #2 DETECTED!\")\n",
        "    print(f\"The error suggests a dtype mismatch.\")\n",
        "    print(f\"\\\\n\ud83d\udca1 Fix hint: In dataloader.py, check the dtype of inputs_cpu\")\n",
        "    print(f\"It's being set to int64 but then converted to int32, causing issues!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fix Bug #2: Data Type Mismatch\n",
        "\n",
        "Go to `nanochat/dataloader.py` and fix the dtype issue in the `tokenizing_distributed_data_loader` function.\n",
        "\n",
        "Look for where `inputs_cpu` is created - it should use `torch.int32`, not `torch.int64`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# After fixing, reload and test\n",
        "import nanochat.dataloader\n",
        "importlib.reload(nanochat.dataloader)\n",
        "\n",
        "# Recreate the batch getter with fixed code\n",
        "def get_batch_fixed(batch_size=4, seq_len=64):\n",
        "    \"\"\"Get a batch of data for training - with fix applied\"\"\"\n",
        "    data = np.fromfile(tokens_path, dtype=np.uint16)\n",
        "    ix = torch.randint(len(data) - seq_len - 1, (batch_size,))\n",
        "    \n",
        "    from collections import deque\n",
        "    token_buffer = deque()\n",
        "    for i in ix:\n",
        "        token_buffer.extend(data[i:i+seq_len+1].tolist())\n",
        "    \n",
        "    B, T = batch_size, seq_len\n",
        "    needed_tokens = B * T + 1\n",
        "    \n",
        "    tokens = [token_buffer.popleft() for _ in range(needed_tokens)]\n",
        "    scratch = torch.tensor(tokens, dtype=torch.int64, pin_memory=(device == \"cuda\"))\n",
        "    \n",
        "    # FIXED: Use int32 for inputs\n",
        "    inputs_cpu = scratch[:-1].to(dtype=torch.int32)\n",
        "    targets_cpu = scratch[1:]  # Still has bug #3!\n",
        "    \n",
        "    inputs = inputs_cpu.view(B, T).to(device=device, dtype=torch.int32, non_blocking=True)\n",
        "    targets = targets_cpu.view(B, T).to(device=device, dtype=torch.int64, non_blocking=True)\n",
        "    return inputs, targets\n",
        "\n",
        "# Test\n",
        "try:\n",
        "    inputs, targets = get_batch_fixed()\n",
        "    print(\"\u2705 Bug #2 FIXED! Batch created successfully!\")\n",
        "    print(f\"Inputs: {inputs.shape}, dtype: {inputs.dtype}\")\n",
        "    print(f\"Targets: {targets.shape}, dtype: {targets.dtype}\")\n",
        "except Exception as e:\n",
        "    print(f\"\u274c Still having issues: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Train the Model and Discover Bug #3\n",
        "\n",
        "Now let's train the model briefly and see if it learns properly. Bug #3 will cause the model to learn the wrong pattern!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick training loop\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "losses = []\n",
        "\n",
        "print(\"Training for 100 steps...\")\n",
        "for step in range(100):\n",
        "    inputs, targets = get_batch_fixed()\n",
        "    \n",
        "    # Forward pass\n",
        "    loss = model(inputs, targets)\n",
        "    \n",
        "    # Backward pass\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    losses.append(loss.item())\n",
        "    \n",
        "    if step % 20 == 0:\n",
        "        print(f\"Step {step}: loss = {loss.item():.4f}\")\n",
        "\n",
        "# Plot loss\n",
        "plt.plot(losses)\n",
        "plt.xlabel('Step')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Now let's test what the model learned - this will reveal Bug #3!\n",
        "model.eval()\n",
        "\n",
        "# Create a test sequence\n",
        "test_tokens = tokenizer.encode(\"The quick\", prepend=\"<|bos|>\")\n",
        "print(f\"Input tokens: {test_tokens}\")\n",
        "print(f\"Input text: '{tokenizer.decode(test_tokens)}'\")\n",
        "\n",
        "# Get model predictions\n",
        "with torch.no_grad():\n",
        "    input_tensor = torch.tensor([test_tokens], dtype=torch.int32).to(device)\n",
        "    logits = model(input_tensor)\n",
        "    \n",
        "    # Get the predicted tokens\n",
        "    predicted_ids = torch.argmax(logits, dim=-1)\n",
        "    predicted_ids = predicted_ids[0].cpu().tolist()\n",
        "\n",
        "print(f\"\\\\nPredicted tokens: {predicted_ids}\")\n",
        "print(f\"Predicted text: '{tokenizer.decode(predicted_ids)}'\")\n",
        "\n",
        "# Check if predictions match inputs (they shouldn't!)\n",
        "if predicted_ids[:len(test_tokens)-1] == test_tokens[:-1]:\n",
        "    print(\"\\\\n\u274c BUG #3 DETECTED! Model is predicting current token instead of next token!\")\n",
        "    print(\"\ud83d\udca1 This means targets = inputs, not inputs shifted by 1\")\n",
        "    print(\"\ud83d\udca1 Check the dataloader where targets are created from scratch\")\n",
        "else:\n",
        "    print(\"\\\\n\u2705 Model correctly predicting next tokens!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fix Bug #3: Off-by-One Error\n",
        "\n",
        "The model is learning to predict the current token instead of the next token!\n",
        "\n",
        "Go to `nanochat/dataloader.py` and fix the targets creation:\n",
        "- Currently: `targets_cpu = scratch[:-1]` (WRONG - same as inputs!)\n",
        "- Should be: `targets_cpu = scratch[1:]` (shifted by 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final test with all fixes\n",
        "importlib.reload(nanochat.dataloader)\n",
        "\n",
        "# Create fully fixed batch getter\n",
        "def get_batch_fully_fixed(batch_size=4, seq_len=64):\n",
        "    \"\"\"Get a batch of data for training - all bugs fixed\"\"\"\n",
        "    data = np.fromfile(tokens_path, dtype=np.uint16)\n",
        "    ix = torch.randint(len(data) - seq_len - 1, (batch_size,))\n",
        "    \n",
        "    from collections import deque\n",
        "    token_buffer = deque()\n",
        "    for i in ix:\n",
        "        token_buffer.extend(data[i:i+seq_len+1].tolist())\n",
        "    \n",
        "    B, T = batch_size, seq_len\n",
        "    needed_tokens = B * T + 1\n",
        "    \n",
        "    tokens = [token_buffer.popleft() for _ in range(needed_tokens)]\n",
        "    scratch = torch.tensor(tokens, dtype=torch.int64, pin_memory=(device == \"cuda\"))\n",
        "    \n",
        "    # ALL FIXES APPLIED:\n",
        "    inputs_cpu = scratch[:-1].to(dtype=torch.int32)  # Fixed: int32\n",
        "    targets_cpu = scratch[1:]  # Fixed: shifted by 1\n",
        "    \n",
        "    inputs = inputs_cpu.view(B, T).to(device=device, dtype=torch.int32, non_blocking=True)\n",
        "    targets = targets_cpu.view(B, T).to(device=device, dtype=torch.int64, non_blocking=True)\n",
        "    return inputs, targets\n",
        "\n",
        "# Verify the fix\n",
        "inputs, targets = get_batch_fully_fixed(batch_size=1, seq_len=10)\n",
        "print(\"Input tokens:\", inputs[0].cpu().tolist())\n",
        "print(\"Target tokens:\", targets[0].cpu().tolist())\n",
        "print(\"\\\\nTargets should be inputs shifted by 1:\")\n",
        "print(\"\u2705 Correct!\" if inputs[0, 1:].cpu().tolist() == targets[0, :-1].cpu().tolist() else \"\u274c Still wrong!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Retrain with all fixes applied\n",
        "print(\"\ud83c\udf89 All bugs fixed! Let's train a working model!\\\\n\")\n",
        "\n",
        "# Reinitialize model\n",
        "model = GPT(config)\n",
        "model.init_weights()\n",
        "model = model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n",
        "losses = []\n",
        "for step in range(200):\n",
        "    inputs, targets = get_batch_fully_fixed()\n",
        "    loss = model(inputs, targets)\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    losses.append(loss.item())\n",
        "    \n",
        "    if step % 40 == 0:\n",
        "        print(f\"Step {step}: loss = {loss.item():.4f}\")\n",
        "\n",
        "plt.plot(losses)\n",
        "plt.xlabel('Step')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss (All Bugs Fixed)')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\\\n\u2705 Success! The model is now training correctly!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final generation test\n",
        "from nanochat.engine import Engine\n",
        "\n",
        "model.eval()\n",
        "engine = Engine(model, tokenizer)\n",
        "\n",
        "# Generate some text\n",
        "prompt = \"The quick\"\n",
        "prompt_tokens = tokenizer.encode(prompt, prepend=\"<|bos|>\")\n",
        "\n",
        "print(f\"Prompt: '{prompt}'\")\n",
        "print(f\"Generating...\\\\n\")\n",
        "\n",
        "generated, _ = engine.generate_batch(prompt_tokens, num_samples=1, max_tokens=20, temperature=0.8)\n",
        "generated_text = tokenizer.decode(generated[0])\n",
        "\n",
        "print(f\"Generated: '{generated_text}'\")\n",
        "print(\"\\\\n\ud83c\udf89 Congratulations! You've fixed all three bugs!\")\n",
        "print(\"\\\\nSummary of fixes:\")\n",
        "print(\"1. \u2705 BOS token prepending in tokenizer.encode()\")\n",
        "print(\"2. \u2705 Data type mismatch (int64 \u2192 int32) in dataloader\")\n",
        "print(\"3. \u2705 Off-by-one error in target creation\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}