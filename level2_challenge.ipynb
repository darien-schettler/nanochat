{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Level 2: Training Pipeline\n\n",
        "**Objective:** Fix bugs in gradient accumulation, learning rate scheduling, optimizer configuration, and SFT masking.\n\n",
        "**Acceptance Criteria:**\n",
        "- All tests in `tests/test_level2.py` pass\n",
        "- Gradient accumulation produces stable training\n",
        "- Learning rate warmup functions correctly\n",
        "- Optimizers are assigned to correct parameter groups\n",
        "- SFT masking trains on assistant responses only\n\n",
        "**Time estimate:** 1-2 hours"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n\n",
        "repo_root = Path.cwd()\n",
        "if str(repo_root) not in sys.path:\n",
        "    sys.path.append(str(repo_root))\n\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Device: {device}\")\n\n",
        "os.environ[\"NANOCHAT_BASE_DIR\"] = os.path.join(repo_root, \".cache_level2\")\n",
        "os.makedirs(os.environ[\"NANOCHAT_BASE_DIR\"], exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 1: Gradient Accumulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a simple model for testing\n",
        "model = torch.nn.Linear(10, 10).to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n\n",
        "# Test gradient accumulation with different accumulation steps\n",
        "def train_with_accum(grad_accum_steps, num_steps=20):\n",
        "    model_copy = torch.nn.Linear(10, 10).to(device)\n",
        "    model_copy.load_state_dict(model.state_dict())\n",
        "    opt = torch.optim.SGD(model_copy.parameters(), lr=0.01)\n",
        "    \n",
        "    losses = []\n",
        "    for step in range(num_steps):\n",
        "        for micro_step in range(grad_accum_steps):\n",
        "            x = torch.randn(2, 10).to(device)\n",
        "            y = torch.randn(2, 10).to(device)\n",
        "            \n",
        "            pred = model_copy(x)\n",
        "            loss = torch.nn.functional.mse_loss(pred, y)\n",
        "            \n",
        "            # Normalize loss by grad_accum_steps\n",
        "            loss = loss / grad_accum_steps\n",
        "            loss.backward()\n",
        "            \n",
        "            losses.append(loss.item() * grad_accum_steps)\n",
        "        \n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "    \n",
        "    return losses\n\n",
        "# Compare different accumulation steps\n",
        "losses_1 = train_with_accum(1)\n",
        "losses_4 = train_with_accum(4)\n",
        "losses_8 = train_with_accum(8)\n\n",
        "print(f\"Loss variance (accum=1): {np.var(losses_1):.6f}\")\n",
        "print(f\"Loss variance (accum=4): {np.var(losses_4):.6f}\")\n",
        "print(f\"Loss variance (accum=8): {np.var(losses_8):.6f}\")\n\n",
        "# Acceptance: Higher accumulation should not cause instability\n",
        "assert np.var(losses_8) < np.var(losses_1) * 100, \"FAIL: Gradient accumulation causes instability\"\n",
        "assert all(np.isfinite(l) for l in losses_8), \"FAIL: Loss contains NaN/Inf\"\n",
        "print(\"\u2713 Test 1 passed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 2: Learning Rate Schedule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test LR warmup schedule\n",
        "def get_lr_multiplier(it, num_iterations=1000, warmup_ratio=0.1):\n",
        "    warmup_iters = round(warmup_ratio * num_iterations)\n",
        "    if it < warmup_iters:\n",
        "        return (it + 1) / warmup_iters\n",
        "    return 1.0\n\n",
        "num_iters = 200\n",
        "warmup_ratio = 0.1\n",
        "warmup_steps = int(warmup_ratio * num_iters)\n\n",
        "lrs = [get_lr_multiplier(i, num_iters, warmup_ratio) for i in range(num_iters)]\n\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(lrs)\n",
        "plt.axvline(x=warmup_steps, color='r', linestyle='--', label='End of warmup')\n",
        "plt.axhline(y=1.0, color='g', linestyle='--', label='Target LR')\n",
        "plt.xlabel('Step')\n",
        "plt.ylabel('LR Multiplier')\n",
        "plt.title('Learning Rate Schedule')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n\n",
        "# Acceptance tests\n",
        "assert lrs[0] > 0.001, f\"FAIL: Initial LR too small: {lrs[0]}\"\n",
        "assert lrs[warmup_steps-1] < 1.0, \"FAIL: Warmup should not complete before warmup_steps\"\n",
        "assert abs(lrs[warmup_steps] - 1.0) < 0.01, f\"FAIL: LR should be ~1.0 after warmup, got {lrs[warmup_steps]}\"\n",
        "assert lrs[warmup_steps//2] > 0.4 and lrs[warmup_steps//2] < 0.6, f\"FAIL: Mid-warmup LR should be ~0.5, got {lrs[warmup_steps//2]}\"\n",
        "print(\"\u2713 Test 2 passed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 3: Optimizer Assignment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nanochat.gpt import GPT, GPTConfig\n",
        "from nanochat.tokenizer import RustBPETokenizer\n\n",
        "# Create tokenizer\n",
        "tokenizer_dir = Path(os.environ[\"NANOCHAT_BASE_DIR\"]) / \"tokenizer\"\n",
        "if not tokenizer_dir.exists():\n",
        "    texts = [\"test\"] * 100\n",
        "    tokenizer = RustBPETokenizer.train_from_iterator(iter(texts), vocab_size=256)\n",
        "    tokenizer.save(str(tokenizer_dir))\n",
        "else:\n",
        "    tokenizer = RustBPETokenizer.from_directory(str(tokenizer_dir))\n\n",
        "# Create model\n",
        "config = GPTConfig(\n",
        "    sequence_len=64,\n",
        "    vocab_size=256,\n",
        "    n_layer=2,\n",
        "    n_head=2,\n",
        "    n_kv_head=2,\n",
        "    n_embd=64,\n",
        ")\n\n",
        "model = GPT(config)\n",
        "model.init_weights()\n",
        "model = model.to(device)\n\n",
        "# Get optimizers\n",
        "optimizers = model.setup_optimizers()\n",
        "adamw_opt, muon_opt = optimizers\n\n",
        "# Check parameter assignments\n",
        "adamw_param_names = set()\n",
        "for group in adamw_opt.param_groups:\n",
        "    for p in group['params']:\n",
        "        for name, param in model.named_parameters():\n",
        "            if param is p:\n",
        "                adamw_param_names.add(name)\n\n",
        "muon_param_names = set()\n",
        "for p in muon_opt.param_groups[0]['params']:\n",
        "    for name, param in model.named_parameters():\n",
        "        if param is p:\n",
        "            muon_param_names.add(name)\n\n",
        "print(f\"AdamW params: {sorted(adamw_param_names)}\")\n",
        "print(f\"Muon params: {sorted(muon_param_names)}\")\n\n",
        "# Acceptance tests\n",
        "assert 'lm_head.weight' in adamw_param_names, \"FAIL: lm_head should use AdamW\"\n",
        "assert 'transformer.wte.weight' in adamw_param_names, \"FAIL: embeddings should use AdamW\"\n",
        "assert any('transformer.h' in name for name in muon_param_names), \"FAIL: transformer layers should use Muon\"\n",
        "assert not any('transformer.h' in name for name in adamw_param_names), \"FAIL: transformer layers should not use AdamW\"\n",
        "print(\"\u2713 Test 3 passed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 4: SFT Masking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test conversation masking\n",
        "conversation = {\n",
        "    \"messages\": [\n",
        "        {\"role\": \"user\", \"content\": \"Hello\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"Hi there!\"}\n",
        "    ]\n",
        "}\n\n",
        "ids, mask = tokenizer.render_conversation(conversation)\n\n",
        "print(f\"Total tokens: {len(ids)}\")\n",
        "print(f\"Tokens to train on (mask=1): {sum(mask)}\")\n",
        "print(f\"Tokens to skip (mask=0): {len(mask) - sum(mask)}\")\n\n",
        "# Simulate SFT data processing\n",
        "ids_tensor = torch.tensor(ids, dtype=torch.long)\n",
        "mask_tensor = torch.tensor(mask[1:], dtype=torch.long)\n",
        "targets = ids_tensor[1:].clone()\n\n",
        "# Apply masking (correct logic)\n",
        "targets[mask_tensor == 0] = -1\n\n",
        "# Count training tokens\n",
        "num_train = (targets != -1).sum().item()\n",
        "num_masked = (targets == -1).sum().item()\n\n",
        "print(f\"\\nAfter masking:\")\n",
        "print(f\"Training on {num_train} tokens\")\n",
        "print(f\"Masked out {num_masked} tokens\")\n\n",
        "# Acceptance: Should train on assistant tokens (mask=1), not user tokens (mask=0)\n",
        "assert num_train > 0, \"FAIL: No tokens to train on\"\n",
        "assert num_masked > 0, \"FAIL: No tokens masked out\"\n",
        "# Assistant responses should be trained on (more than half of non-BOS tokens)\n",
        "assert num_train > len(mask) * 0.2, f\"FAIL: Too few training tokens ({num_train}/{len(mask)})\"\n",
        "print(\"\u2713 Test 4 passed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n\n",
        "All tests passed! Training pipeline is functioning correctly."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}