{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Level 1: Tokenization & Data Pipeline\n\n",
        "**Objective:** Fix bugs in the tokenization and data loading pipeline.\n\n",
        "**Acceptance Criteria:**\n",
        "- All tests in `tests/test_level1.py` pass\n",
        "- Model trains without errors\n",
        "- Training loss decreases consistently\n\n",
        "**Time estimate:** 30-45 minutes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n\n",
        "repo_root = Path.cwd()\n",
        "if str(repo_root) not in sys.path:\n",
        "    sys.path.append(str(repo_root))\n\n",
        "device = 'cuda' if torch.cuda.is_available() else 'mps' if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available() else 'cpu'\n",
        "print(f\"Device: {device}\")\n\n",
        "os.environ[\"NANOCHAT_BASE_DIR\"] = os.path.join(repo_root, \".cache_level1\")\n",
        "os.makedirs(os.environ[\"NANOCHAT_BASE_DIR\"], exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup: Train Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nanochat.tokenizer import RustBPETokenizer\n\n",
        "tokenizer_dir = Path(os.environ[\"NANOCHAT_BASE_DIR\"]) / \"tokenizer\"\n\n",
        "if not tokenizer_dir.exists():\n",
        "    texts = [\"The quick brown fox jumps over the lazy dog.\"] * 1000\n",
        "    tokenizer = RustBPETokenizer.train_from_iterator(iter(texts), vocab_size=512)\n",
        "    tokenizer.save(str(tokenizer_dir))\n",
        "else:\n",
        "    tokenizer = RustBPETokenizer.from_directory(str(tokenizer_dir))\n\n",
        "print(f\"Tokenizer ready: {tokenizer.get_vocab_size()} tokens\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 1: Tokenizer Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test tokenizer with BOS prepending\n",
        "test_text = \"Hello world\"\n",
        "bos_id = tokenizer.get_bos_token_id()\n\n",
        "tokens_with_bos = tokenizer.encode(test_text, prepend=\"<|bos|>\")\n",
        "tokens_without = tokenizer.encode(test_text)\n\n",
        "print(f\"BOS token ID: {bos_id}\")\n",
        "print(f\"With BOS: {tokens_with_bos[:5]}\")\n",
        "print(f\"Without BOS: {tokens_without[:5]}\")\n\n",
        "# Acceptance test\n",
        "assert tokens_with_bos[0] == bos_id, f\"FAIL: First token should be BOS ({bos_id}), got {tokens_with_bos[0]}\"\n",
        "assert len(tokens_with_bos) == len(tokens_without) + 1, \"FAIL: BOS token not added\"\n",
        "print(\"\u2713 Test 1 passed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 2: Data Loading Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create test data\n",
        "texts = [\"The quick brown fox\"] * 100\n",
        "all_tokens = []\n",
        "for text in texts:\n",
        "    all_tokens.extend(tokenizer.encode(text, prepend=\"<|bos|>\"))\n\n",
        "tokens_path = Path(os.environ[\"NANOCHAT_BASE_DIR\"]) / \"tokens.bin\"\n",
        "np.array(all_tokens, dtype=np.uint16).tofile(tokens_path)\n",
        "print(f\"Created test data: {len(all_tokens)} tokens\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test batch creation\n",
        "from collections import deque\n\n",
        "def create_batch(batch_size=2, seq_len=10):\n",
        "    data = np.fromfile(tokens_path, dtype=np.uint16)\n",
        "    token_buffer = deque(data[:batch_size * seq_len + 1].tolist())\n",
        "    \n",
        "    B, T = batch_size, seq_len\n",
        "    needed_tokens = B * T + 1\n",
        "    tokens = [token_buffer.popleft() for _ in range(needed_tokens)]\n",
        "    scratch = torch.tensor(tokens, dtype=torch.int64)\n",
        "    \n",
        "    # This mimics dataloader logic\n",
        "    inputs_cpu = scratch[:-1].to(dtype=torch.int32)\n",
        "    targets_cpu = scratch[1:]\n",
        "    \n",
        "    inputs = inputs_cpu.view(B, T).to(device=device, dtype=torch.int32)\n",
        "    targets = targets_cpu.view(B, T).to(device=device, dtype=torch.int64)\n",
        "    return inputs, targets\n\n",
        "try:\n",
        "    inputs, targets = create_batch()\n",
        "    print(f\"Inputs: {inputs.shape}, dtype={inputs.dtype}\")\n",
        "    print(f\"Targets: {targets.shape}, dtype={targets.dtype}\")\n",
        "    \n",
        "    # Acceptance tests\n",
        "    assert inputs.dtype == torch.int32, f\"FAIL: inputs should be int32, got {inputs.dtype}\"\n",
        "    assert targets.dtype == torch.int64, f\"FAIL: targets should be int64, got {targets.dtype}\"\n",
        "    \n",
        "    # Verify autoregressive property: targets should be inputs shifted by 1\n",
        "    inputs_flat = inputs.flatten().cpu()\n",
        "    targets_flat = targets.flatten().cpu()\n",
        "    assert not torch.equal(inputs_flat, targets_flat), \"FAIL: targets should not equal inputs\"\n",
        "    assert torch.equal(inputs_flat[1:], targets_flat[:-1]), \"FAIL: targets should be inputs shifted by 1\"\n",
        "    \n",
        "    print(\"\u2713 Test 2 passed\")\n",
        "except Exception as e:\n",
        "    print(f\"\u2717 Test 2 failed: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 3: Training Smoke Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nanochat.gpt import GPT, GPTConfig\n\n",
        "config = GPTConfig(\n",
        "    sequence_len=64,\n",
        "    vocab_size=tokenizer.get_vocab_size(),\n",
        "    n_layer=2,\n",
        "    n_head=2,\n",
        "    n_kv_head=2,\n",
        "    n_embd=64,\n",
        ")\n\n",
        "model = GPT(config)\n",
        "model.init_weights()\n",
        "model = model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n\n",
        "print(f\"Model: {sum(p.numel() for p in model.parameters()):,} parameters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train for a few steps\n",
        "losses = []\n",
        "for step in range(50):\n",
        "    inputs, targets = create_batch(batch_size=4, seq_len=32)\n",
        "    loss = model(inputs, targets)\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    losses.append(loss.item())\n",
        "    if step % 10 == 0:\n",
        "        print(f\"Step {step}: loss = {loss.item():.4f}\")\n\n",
        "# Acceptance test: loss should decrease\n",
        "initial_loss = np.mean(losses[:10])\n",
        "final_loss = np.mean(losses[-10:])\n",
        "improvement = (initial_loss - final_loss) / initial_loss\n\n",
        "print(f\"\\nInitial loss: {initial_loss:.4f}\")\n",
        "print(f\"Final loss: {final_loss:.4f}\")\n",
        "print(f\"Improvement: {improvement*100:.1f}%\")\n\n",
        "assert improvement > 0.1, f\"FAIL: Loss should improve by >10%, got {improvement*100:.1f}%\"\n",
        "print(\"\u2713 Test 3 passed\")\n\n",
        "plt.plot(losses)\n",
        "plt.xlabel('Step')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n\n",
        "All tests passed! The tokenization and data loading pipeline is working correctly."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}