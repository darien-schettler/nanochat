{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83d\udc1b Nanochat Bug Hunt: Architecture & Performance\n\nWelcome to the advanced debugging challenge! This notebook contains 5 challenging bugs:\n\n1. **Rotary Embedding Bug**: Model can't handle long sequences\n2. **KV Cache Corruption**: Garbage output after ~50 tokens\n3. **MQA Implementation Bug**: Wrong condition for Group Query Attention\n4. **Memory Leak**: KV cache grows without bounds\n5. **Tool Use Bug**: Calculator results appear at wrong positions\n\nThese bugs require deep understanding of transformer architecture and modern optimizations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup\nimport os\nimport sys\nimport torch\nimport numpy as np\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport importlib\nimport gc\n\n# Add nanochat to path\nrepo_root = Path.cwd()\nif str(repo_root) not in sys.path:\n    sys.path.append(str(repo_root))\n\n# Device selection\ndevice = 'cuda' if torch.cuda.is_available() else 'mps' if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available() else 'cpu'\nprint(f\"Using device: {device}\")\n\n# Set up cache directory\nos.environ[\"NANOCHAT_BASE_DIR\"] = os.path.join(repo_root, \".cache_hard\")\nos.makedirs(os.environ[\"NANOCHAT_BASE_DIR\"], exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preparation: Create Model and Tokenizer\n\nWe'll need a model to test these advanced bugs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create or load tokenizer\nfrom nanochat.tokenizer import RustBPETokenizer\n\ntokenizer_dir = Path(os.environ[\"NANOCHAT_BASE_DIR\"]) / \"tokenizer\"\n\nif tokenizer_dir.exists():\n    print(\"Loading tokenizer...\")\n    tokenizer = RustBPETokenizer.from_directory(str(tokenizer_dir))\nelse:\n    print(\"Training tokenizer...\")\n    texts = [\"The quick brown fox\"] * 1000\n    tokenizer = RustBPETokenizer.train_from_iterator(iter(texts), vocab_size=1024)\n    tokenizer.save(str(tokenizer_dir))\n\nprint(f\"Vocab size: {tokenizer.get_vocab_size()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a model with specific config to test bugs\nfrom nanochat.gpt import GPT, GPTConfig\n\nconfig = GPTConfig(\n    sequence_len=256,  # Long enough to test position issues\n    vocab_size=tokenizer.get_vocab_size(),\n    n_layer=4,\n    n_head=8,       # 8 query heads\n    n_kv_head=4,    # 4 key/value heads for MQA\n    n_embd=256,\n)\n\nmodel = GPT(config)\nmodel.init_weights()\nmodel = model.to(device)\nprint(f\"Model: {sum(p.numel() for p in model.parameters()):,} parameters\")\nprint(f\"Using MQA: {config.n_head} query heads, {config.n_kv_head} kv heads\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bug #1: Rotary Embedding Frequency Calculation\n\nLet's test if the model can handle sequences at different positions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test rotary embeddings at different positions\ndef test_rotary_positions(model, positions=[0, 50, 100, 150, 200]):\n    \"\"\"Test if model produces consistent outputs at different sequence positions\"\"\"\n    \n    # Create a simple input\n    test_seq = torch.randint(0, 100, (1, 10), dtype=torch.int32).to(device)\n    \n    outputs = []\n    for pos_offset in positions:\n        # Simulate being at different positions in sequence\n        with torch.no_grad():\n            # Get rotary embeddings for this position\n            T = test_seq.size(1)\n            cos_sin = model.cos[:, pos_offset:pos_offset+T], model.sin[:, pos_offset:pos_offset+T]\n            \n            # Manual forward through first attention layer\n            x = model.transformer.wte(test_seq)\n            x = torch.nn.functional.rms_norm(x, (x.size(-1),))\n            \n            # Get attention output\n            attn_out = model.transformer.h[0].attn(x, cos_sin, kv_cache=None)\n            outputs.append(attn_out[0, 0, :5].cpu().numpy())  # First 5 dims\n    \n    return outputs\n\n# Test at different positions\npositions = [0, 50, 100, 150, 200]\noutputs = test_rotary_positions(model, positions)\n\n# Plot the outputs\nplt.figure(figsize=(10, 6))\nfor i, (pos, out) in enumerate(zip(positions, outputs)):\n    plt.plot(out, label=f'Position {pos}', marker='o')\n\nplt.xlabel('Dimension')\nplt.ylabel('Output Value')\nplt.title('Attention Output at Different Sequence Positions')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Check variance\nvariances = [np.var(out) for out in outputs]\nprint(f\"Output variances: {variances}\")\nif variances[-1] < variances[0] * 0.1:\n    print(\"\\\\n\u274c BUG #1 DETECTED! Outputs degrade at higher positions!\")\n    print(\"\ud83d\udca1 The rotary frequency calculation seems wrong.\")\n    print(\"\ud83d\udca1 Check _precompute_rotary_embeddings() in gpt.py\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fix Bug #1: Rotary Embeddings\n\nThe frequency calculation is wrong:\n- Current: `inv_freq = 1.0 / (base ** (channel_range / (head_dim * 2)))`\n- Should be: `inv_freq = 1.0 / (base ** (channel_range / head_dim))`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# After fixing, test again\nprint(\"After fixing the rotary embedding calculation...\")\nprint(\"\u2705 Model should handle all positions equally well!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bug #2 & #4: KV Cache Position Tracking and Memory Leak\n\nLet's test generation with the KV cache."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test KV cache during generation\nfrom nanochat.engine import Engine, KVCache\n\n# Create engine\nengine = Engine(model, tokenizer)\n\n# Generate some text and monitor KV cache\ndef test_generation_with_cache_monitoring():\n    prompt = \"Once upon a time\"\n    prompt_tokens = tokenizer.encode(prompt, prepend=\"<|bos|>\")\n    \n    print(f\"Generating from: '{prompt}'\")\n    print(\"\\\\nMonitoring KV cache positions...\\\\n\")\n    \n    # Track positions\n    positions = []\n    tokens_generated = []\n    \n    # Generate token by token to monitor\n    for i, (token_col, masks) in enumerate(engine.generate(prompt_tokens, max_tokens=20)):\n        token = token_col[0]\n        tokens_generated.append(token)\n        \n        # Try to access cache position (this will fail with the bug)\n        if i < 10:  # Only print first 10\n            print(f\"Step {i}: Generated token {token} ('{tokenizer.decode([token])}')\") \n    \n    # Decode all\n    generated_text = tokenizer.decode(prompt_tokens + tokens_generated)\n    print(f\"\\\\nGenerated text: '{generated_text}'\")\n    \n    return generated_text\n\n# Test generation\ngenerated = test_generation_with_cache_monitoring()\n\n# Check for corruption\nif len(set(generated.split()[-5:])) < 3:  # Last 5 words are too repetitive\n    print(\"\\\\n\u274c BUG #2 DETECTED! Generation quality degrades!\")\n    print(\"\ud83d\udca1 KV cache position tracking is incrementing at the wrong layer.\")\n    print(\"\ud83d\udca1 Check insert_kv() in engine.py - it should increment after LAST layer.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test memory usage with multiple generations\ndef test_memory_leak():\n    print(\"Testing memory usage across multiple generations...\\\\n\")\n    \n    # Create a fresh engine\n    engine = Engine(model, tokenizer)\n    \n    mem_usage = []\n    \n    for i in range(5):\n        # Generate some text\n        tokens = tokenizer.encode(f\"Test {i}:\", prepend=\"<|bos|>\")\n        result, _ = engine.generate_batch(tokens, max_tokens=50)\n        \n        # Force garbage collection and measure memory\n        gc.collect()\n        if device == 'cuda':\n            torch.cuda.empty_cache()\n            mem = torch.cuda.memory_allocated() / 1024**2  # MB\n        else:\n            mem = 0  # Can't easily measure on CPU/MPS\n        \n        mem_usage.append(mem)\n        print(f\"Generation {i+1}: Memory used: {mem:.2f} MB\")\n    \n    if device == 'cuda' and len(mem_usage) > 1:\n        mem_increase = mem_usage[-1] - mem_usage[0]\n        print(f\"\\\\nMemory increase: {mem_increase:.2f} MB\")\n        \n        if mem_increase > 50:  # More than 50MB increase is suspicious\n            print(\"\\\\n\u274c BUG #4 DETECTED! Memory usage keeps growing!\")\n            print(\"\ud83d\udca1 KV cache is not being properly reset between generations.\")\n            print(\"\ud83d\udca1 Check reset() method in KVCache class.\")\n            print(\"\ud83d\udca1 Also check the cache growth logic - it's growing 2x more than needed!\")\n\ntest_memory_leak()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bug #3: MQA Implementation\n\nTest Multi-Query Attention with different head configurations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test MQA implementation\ndef test_mqa_shapes():\n    \"\"\"Test if MQA works correctly with n_head != n_kv_head\"\"\"\n    \n    # Our config has n_head=8, n_kv_head=4\n    print(f\"Testing MQA: {config.n_head} query heads, {config.n_kv_head} kv heads\")\n    print(f\"enable_gqa should be True when n_head != n_kv_head\\\\n\")\n    \n    # Check the attention layer\n    attn_layer = model.transformer.h[0].attn\n    \n    # Create dummy input\n    B, T = 2, 10\n    x = torch.randn(B, T, config.n_embd).to(device)\n    \n    # Get Q, K, V\n    q = attn_layer.c_q(x).view(B, T, config.n_head, config.n_embd // config.n_head)\n    k = attn_layer.c_k(x).view(B, T, config.n_kv_head, config.n_embd // config.n_head)\n    v = attn_layer.c_v(x).view(B, T, config.n_kv_head, config.n_embd // config.n_head)\n    \n    print(f\"Query shape: {q.shape} - (batch, seq, n_heads={config.n_head}, head_dim)\")\n    print(f\"Key shape:   {k.shape} - (batch, seq, n_kv_heads={config.n_kv_head}, head_dim)\")\n    print(f\"Value shape: {v.shape} - (batch, seq, n_kv_heads={config.n_kv_head}, head_dim)\")\n    \n    # Test the enable_gqa logic\n    enable_gqa_buggy = config.n_head == config.n_kv_head  # Bug version\n    enable_gqa_correct = config.n_head != config.n_kv_head  # Correct version\n    \n    print(f\"\\\\nBuggy enable_gqa: {enable_gqa_buggy}\")\n    print(f\"Correct enable_gqa: {enable_gqa_correct}\")\n    \n    if enable_gqa_buggy != enable_gqa_correct:\n        print(\"\\\\n\u274c BUG #3 DETECTED! MQA condition is inverted!\")\n        print(\"\ud83d\udca1 enable_gqa should be True when n_head != n_kv_head\")\n        print(\"\ud83d\udca1 Fix in forward() method of CausalSelfAttention\")\n\ntest_mqa_shapes()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bug #5: Tool Use State Machine\n\nTest the calculator tool integration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test calculator tool\ndef test_calculator_tool():\n    print(\"Testing calculator tool integration...\\\\n\")\n    \n    # Create a conversation that uses the calculator\n    conversation = {\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"What is 25 + 17?\"},\n            {\n                \"role\": \"assistant\",\n                \"content\": [\n                    {\"type\": \"text\", \"text\": \"Let me calculate that: \"},\n                    {\"type\": \"python\", \"text\": \"25 + 17\"},\n                    {\"type\": \"python_output\", \"text\": \"42\"},\n                    {\"type\": \"text\", \"text\": \" The answer is 42.\"}\n                ]\n            }\n        ]\n    }\n    \n    # Tokenize to see the structure\n    ids, mask = tokenizer.render_conversation(conversation)\n    \n    # Visualize the tokenization\n    print(\"Tokenized conversation (first 100 tokens):\")\n    print(tokenizer.visualize_tokenization(ids[:100], mask[:100]))\n    \n    # Now test generation with calculator\n    engine = Engine(model, tokenizer)\n    \n    # Prepare a prompt that triggers calculator use\n    calc_prompt = tokenizer.encode(\"Calculate 10 + 15: <|python_start|>10 + 15<|python_end|>\")\n    \n    print(\"\\\\n\\\\nTesting calculator execution...\")\n    print(\"Input includes: <|python_start|>10 + 15<|python_end|>\")\n    print(\"Expected: <|output_start|>25<|output_end|>\\\\n\")\n    \n    # Generate and track tokens\n    generated_tokens = []\n    special_tokens = []\n    \n    for token_col, masks in engine.generate(calc_prompt, max_tokens=20):\n        token = token_col[0]\n        generated_tokens.append(token)\n        \n        # Check for special tokens\n        decoded = tokenizer.decode([token])\n        if decoded.startswith('<|') and decoded.endswith('|>'):\n            special_tokens.append((len(generated_tokens)-1, decoded))\n    \n    print(\"Special tokens found:\")\n    for pos, tok in special_tokens:\n        print(f\"  Position {pos}: {tok}\")\n    \n    # Check order\n    if len(special_tokens) >= 2:\n        if special_tokens[0][1] != '<|output_start|>':\n            print(\"\\\\n\u274c BUG #5 DETECTED! Tool output tokens in wrong order!\")\n            print(\"\ud83d\udca1 Calculator result injection is messed up.\")\n            print(\"\ud83d\udca1 Check the order of output_start and output_end in engine.py\")\n    \n    # Show full output\n    full_output = tokenizer.decode(calc_prompt + generated_tokens)\n    print(f\"\\\\nFull output: '{full_output}'\")\n\ntest_calculator_tool()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n\nAfter fixing all bugs:\n\n1. **Rotary Embeddings**: Fix frequency calculation (remove *2)\n2. **KV Cache Position**: Increment after last layer, not first\n3. **MQA Condition**: Use != instead of == for enable_gqa\n4. **Memory Leak**: Reset kv_cache in reset(), fix growth factor\n5. **Tool Output**: Correct order: output_start, tokens, output_end\n\nThese bugs demonstrate the complexity of modern transformer optimizations!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\ud83c\udf89 Congratulations on completing the advanced architecture challenge!\\\\n\")\nprint(\"You've debugged:\")\nprint(\"- Positional encoding mathematics\")\nprint(\"- Stateful caching mechanisms\")\nprint(\"- Attention optimization techniques\")\nprint(\"- Memory management\")\nprint(\"- Tool integration state machines\")\nprint(\"\\\\nThese are real issues that can occur in production LLM code!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}